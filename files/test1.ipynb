{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fd831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "path = os.path.dirname(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2269dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    content = content.lower().splitlines()\n",
    "    content = [line.split()[0] for line in content if line]\n",
    "    return content\n",
    "\n",
    "def extract_sent(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    content = content.lower().replace('\\n', ' ')\n",
    "    content = re.sub(r'https://+\\S+', '', content)\n",
    "    sentences = re.split(r'[.?!:;]\\s+', content)\n",
    "    # content = re.findall(r'\\b\\w+\\b', content)\n",
    "    return sentences\n",
    "\n",
    "def text_analysis(sent_list, positive_words, negative_words, stop_words):\n",
    "    word_list = []\n",
    "    for sent in sent_list:\n",
    "        words = re.findall(r'\\b\\w+\\b', sent)\n",
    "        word_list.extend(words)\n",
    "    positive_words = list(set(positive_words)-set(stop_words))\n",
    "    negative_words = list(set(negative_words)-set(stop_words))\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in word_list:\n",
    "        if word in positive_words:\n",
    "            pos += 1\n",
    "        elif word in negative_words:\n",
    "            neg += 1\n",
    "    polarity_score = (pos - neg)/ (pos + neg + 1e-6)\n",
    "    cleaned_words = [word for word in word_list if word not in stop_words]\n",
    "    subjectivity_score = (pos + neg) / (len(cleaned_words) + 1e-6)\n",
    "\n",
    "    def syllable_count(word):\n",
    "        word = word.lower()\n",
    "        count = 0\n",
    "        vowels = \"aeiouy\"\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"e\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        return count\n",
    "    \n",
    "    complex_words = 0\n",
    "    for word in word_list:\n",
    "        if syllable_count(word) >= 3 and word:\n",
    "            complex_words += 1\n",
    "    avg_sent_length = len(word_list) / len(sent_list)\n",
    "    perc_complex_words = complex_words / len(word_list)\n",
    "    gunning_fog_index = 0.4 * (avg_sent_length + perc_complex_words)\n",
    "    avg_syllables_per_word = sum(syllable_count(word) for word in word_list) / (len(word_list) + 1e-6)\n",
    "    avg_word_length = sum(len(word) for word in word_list) / (len(word_list) + 1e-6)\n",
    "    avg_sent_length = len(word_list) / (len(sent_list) + 1e-6)\n",
    "    num_personal_pronouns = sum(1 for word in word_list if word in ['i', 'my', 'we', 'us', 'our'])\n",
    "\n",
    "    return {\n",
    "        'POSITIVE SCORE': pos,\n",
    "        'NEGATIVE SCORE': neg,\n",
    "        'FOG INDEX': gunning_fog_index,\n",
    "        'POLARITY SCORE': polarity_score,\n",
    "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "        'SYLLABLE PER WORD': avg_syllables_per_word,\n",
    "        'AVG SENTENCE LENGTH': avg_sent_length,\n",
    "        'AVG WORD LENGTH': avg_word_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': perc_complex_words,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': avg_sent_length,\n",
    "        'PERSONAL PRONOUNS': num_personal_pronouns,\n",
    "        'COMPLEX WORD COUNT': complex_words,\n",
    "        'WORD COUNT': len(word_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d3eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stopwords = []\n",
    "for file in os.listdir(os.path.join(path, '20211030 Test Assignment\\\\StopWords')):\n",
    "    list_stopwords.append(extract_words(os.path.join(path, f'20211030 Test Assignment\\\\StopWords\\\\{file}')))\n",
    "\n",
    "stop_words = []\n",
    "for sublist in list_stopwords:\n",
    "    stop_words += sublist\n",
    "stop_words = list(set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b230b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\nitee\\AppData\\Local\\Temp\\ipykernel_21092\\1251399902.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  positive_words = list(set(extract_words(os.path.join(path, '20211030 Test Assignment\\MasterDictionary\\\\positive-words.txt')))-set(stop_words))\n",
      "C:\\Users\\nitee\\AppData\\Local\\Temp\\ipykernel_21092\\1251399902.py:2: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  negative_words = list(set(extract_words(os.path.join(path, '20211030 Test Assignment\\MasterDictionary\\\\negative-words.txt')))-set(stop_words))\n"
     ]
    }
   ],
   "source": [
    "positive_words = list(set(extract_words(os.path.join(path, '20211030 Test Assignment\\MasterDictionary\\\\positive-words.txt')))-set(stop_words))\n",
    "negative_words = list(set(extract_words(os.path.join(path, '20211030 Test Assignment\\MasterDictionary\\\\negative-words.txt')))-set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "623b93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_excel(os.path.join(path, '20211030 Test Assignment\\\\Input.xlsx'), sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8193897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Netclan20241017.txt...\n",
      "Processing Netclan20241018.txt...\n",
      "Processing Netclan20241019.txt...\n",
      "Processing Netclan20241020.txt...\n",
      "Processing Netclan20241021.txt...\n",
      "Processing Netclan20241022.txt...\n",
      "Processing Netclan20241023.txt...\n",
      "Processing Netclan20241024.txt...\n",
      "Processing Netclan20241025.txt...\n",
      "Processing Netclan20241026.txt...\n",
      "Processing Netclan20241027.txt...\n",
      "Processing Netclan20241028.txt...\n",
      "Processing Netclan20241029.txt...\n",
      "Processing Netclan20241030.txt...\n",
      "Processing Netclan20241031.txt...\n",
      "Processing Netclan20241032.txt...\n",
      "Processing Netclan20241033.txt...\n",
      "Processing Netclan20241034.txt...\n",
      "Processing Netclan20241035.txt...\n",
      "Processing Netclan20241036.txt...\n",
      "Processing Netclan20241037.txt...\n",
      "Processing Netclan20241038.txt...\n",
      "Processing Netclan20241039.txt...\n",
      "Processing Netclan20241040.txt...\n",
      "Processing Netclan20241041.txt...\n",
      "Processing Netclan20241042.txt...\n",
      "Processing Netclan20241043.txt...\n",
      "Processing Netclan20241044.txt...\n",
      "Processing Netclan20241045.txt...\n",
      "Processing Netclan20241046.txt...\n",
      "Processing Netclan20241047.txt...\n",
      "Processing Netclan20241048.txt...\n",
      "Processing Netclan20241049.txt...\n",
      "Processing Netclan20241050.txt...\n",
      "Processing Netclan20241051.txt...\n",
      "Processing Netclan20241052.txt...\n",
      "Processing Netclan20241053.txt...\n",
      "Processing Netclan20241054.txt...\n",
      "Processing Netclan20241055.txt...\n",
      "Processing Netclan20241056.txt...\n",
      "Processing Netclan20241057.txt...\n",
      "Processing Netclan20241058.txt...\n",
      "Processing Netclan20241059.txt...\n",
      "Processing Netclan20241060.txt...\n",
      "Processing Netclan20241061.txt...\n",
      "Processing Netclan20241062.txt...\n",
      "Processing Netclan20241063.txt...\n",
      "Processing Netclan20241064.txt...\n",
      "Processing Netclan20241065.txt...\n",
      "Processing Netclan20241066.txt...\n",
      "Processing Netclan20241067.txt...\n",
      "Processing Netclan20241068.txt...\n",
      "Processing Netclan20241069.txt...\n",
      "Processing Netclan20241070.txt...\n",
      "Processing Netclan20241071.txt...\n",
      "Processing Netclan20241072.txt...\n",
      "Processing Netclan20241073.txt...\n",
      "Processing Netclan20241074.txt...\n",
      "Processing Netclan20241075.txt...\n",
      "Processing Netclan20241076.txt...\n",
      "Processing Netclan20241077.txt...\n",
      "Processing Netclan20241078.txt...\n",
      "Processing Netclan20241079.txt...\n",
      "Processing Netclan20241080.txt...\n",
      "Processing Netclan20241081.txt...\n",
      "Processing Netclan20241082.txt...\n",
      "Processing Netclan20241083.txt...\n",
      "Processing Netclan20241084.txt...\n",
      "Processing Netclan20241085.txt...\n",
      "Processing Netclan20241086.txt...\n",
      "Processing Netclan20241087.txt...\n",
      "Processing Netclan20241088.txt...\n",
      "Processing Netclan20241089.txt...\n",
      "Processing Netclan20241090.txt...\n",
      "Processing Netclan20241091.txt...\n",
      "Processing Netclan20241092.txt...\n",
      "Processing Netclan20241093.txt...\n",
      "Processing Netclan20241094.txt...\n",
      "Processing Netclan20241095.txt...\n",
      "Processing Netclan20241096.txt...\n",
      "Processing Netclan20241097.txt...\n",
      "Processing Netclan20241098.txt...\n",
      "Processing Netclan20241099.txt...\n",
      "Processing Netclan20241100.txt...\n",
      "Processing Netclan20241101.txt...\n",
      "Processing Netclan20241102.txt...\n",
      "Processing Netclan20241103.txt...\n",
      "Processing Netclan20241104.txt...\n",
      "Processing Netclan20241105.txt...\n",
      "Processing Netclan20241106.txt...\n",
      "Processing Netclan20241107.txt...\n",
      "Processing Netclan20241108.txt...\n",
      "Processing Netclan20241109.txt...\n",
      "Processing Netclan20241110.txt...\n",
      "Processing Netclan20241111.txt...\n",
      "Processing Netclan20241112.txt...\n",
      "Processing Netclan20241113.txt...\n",
      "Processing Netclan20241114.txt...\n",
      "Processing Netclan20241115.txt...\n",
      "Processing Netclan20241116.txt...\n",
      "Processing Netclan20241117.txt...\n",
      "Processing Netclan20241118.txt...\n",
      "Processing Netclan20241119.txt...\n",
      "Processing Netclan20241120.txt...\n",
      "Processing Netclan20241121.txt...\n",
      "Processing Netclan20241122.txt...\n",
      "Processing Netclan20241123.txt...\n",
      "Processing Netclan20241124.txt...\n",
      "Processing Netclan20241125.txt...\n",
      "Processing Netclan20241126.txt...\n",
      "Processing Netclan20241127.txt...\n",
      "Processing Netclan20241128.txt...\n",
      "Processing Netclan20241129.txt...\n",
      "Processing Netclan20241130.txt...\n",
      "Processing Netclan20241131.txt...\n",
      "Processing Netclan20241132.txt...\n",
      "Processing Netclan20241133.txt...\n",
      "Processing Netclan20241134.txt...\n",
      "Processing Netclan20241135.txt...\n",
      "Processing Netclan20241136.txt...\n",
      "Processing Netclan20241137.txt...\n",
      "Processing Netclan20241138.txt...\n",
      "Processing Netclan20241139.txt...\n",
      "Processing Netclan20241140.txt...\n",
      "Processing Netclan20241141.txt...\n",
      "Processing Netclan20241142.txt...\n",
      "Processing Netclan20241143.txt...\n",
      "Processing Netclan20241144.txt...\n",
      "Processing Netclan20241145.txt...\n",
      "Processing Netclan20241146.txt...\n",
      "Processing Netclan20241147.txt...\n",
      "Processing Netclan20241148.txt...\n",
      "Processing Netclan20241149.txt...\n",
      "Processing Netclan20241150.txt...\n",
      "Processing Netclan20241151.txt...\n",
      "Processing Netclan20241152.txt...\n",
      "Processing Netclan20241153.txt...\n",
      "Processing Netclan20241155.txt...\n",
      "Processing Netclan20241156.txt...\n",
      "Processing Netclan20241157.txt...\n",
      "Processing Netclan20241158.txt...\n",
      "Processing Netclan20241159.txt...\n",
      "Processing Netclan20241160.txt...\n",
      "Processing Netclan20241161.txt...\n",
      "Processing Netclan20241162.txt...\n",
      "Processing Netclan20241163.txt...\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(os.path.join(path, 'Aritcle_txt_files')):\n",
    "    analysis_dict = text_analysis(extract_sent(os.path.join(path, f'Aritcle_txt_files\\\\{file}')),\n",
    "                  positive_words,\n",
    "                  negative_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c3015a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows... : 147it [00:13, 10.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop over each row and fill in the analysis results\n",
    "for i, row in tqdm(output_df.iterrows(), desc=\"Processing rows... \"):\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    \n",
    "    # Load corresponding sentence list for the article (from file or memory)\n",
    "    try:\n",
    "        sent_list = extract_sent(os.path.join(path, f'Aritcle_txt_files\\\\{url_id}.txt'))\n",
    "    except FileNotFoundError:\n",
    "        continue  # Skip if file is missing\n",
    "\n",
    "    # Analyze the text\n",
    "    analysis = text_analysis(sent_list, positive_words, negative_words, stop_words)\n",
    "\n",
    "    # Fill in the DataFrame\n",
    "    for key, value in analysis.items():\n",
    "        if key in output_df.columns:\n",
    "            output_df.at[i, key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a546c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(os.path.join(path, '20211030 Test Assignment\\\\Output.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb5bfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSITIVE SCORE': 88,\n",
       " 'NEGATIVE SCORE': 12,\n",
       " 'FOG INDEX': 5.785023037141514,\n",
       " 'POLARITY SCORE': 0.7599999924,\n",
       " 'SUBJECTIVITY SCORE': 0.07570022704337605,\n",
       " 'SYLLABLE PER WORD': 2.0117536426837077,\n",
       " 'AVG SENTENCE LENGTH': 14.179999905466667,\n",
       " 'AVG WORD LENGTH': 6.056887632319282,\n",
       " 'PERCENTAGE OF COMPLEX WORDS': 0.2825575928537847,\n",
       " 'AVG NUMBER OF WORDS PER SENTENCE': 14.179999905466667,\n",
       " 'PERSONAL PRONOUNS': 1,\n",
       " 'COMPLEX WORD COUNT': 601,\n",
       " 'WORD COUNT': 2127}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = extract_sent(r'C:\\Users\\nitee\\OneDrive\\Desktop\\VS CODE\\BlackCoffer-TestAssginment\\Aritcle_txt_files\\Netclan20241049.txt')\n",
    "text_analysis(sent_list, positive_words, negative_words, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8599da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 147 entries, 0 to 146\n",
      "Data columns (total 15 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   URL_ID                            147 non-null    object \n",
      " 1   URL                               147 non-null    object \n",
      " 2   POSITIVE SCORE                    146 non-null    float64\n",
      " 3   NEGATIVE SCORE                    146 non-null    float64\n",
      " 4   POLARITY SCORE                    146 non-null    float64\n",
      " 5   SUBJECTIVITY SCORE                146 non-null    float64\n",
      " 6   AVG SENTENCE LENGTH               146 non-null    float64\n",
      " 7   PERCENTAGE OF COMPLEX WORDS       146 non-null    float64\n",
      " 8   FOG INDEX                         146 non-null    float64\n",
      " 9   AVG NUMBER OF WORDS PER SENTENCE  146 non-null    float64\n",
      " 10  COMPLEX WORD COUNT                146 non-null    float64\n",
      " 11  WORD COUNT                        146 non-null    float64\n",
      " 12  SYLLABLE PER WORD                 146 non-null    float64\n",
      " 13  PERSONAL PRONOUNS                 146 non-null    float64\n",
      " 14  AVG WORD LENGTH                   146 non-null    float64\n",
      "dtypes: float64(13), object(2)\n",
      "memory usage: 17.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\nitee\\OneDrive\\Desktop\\VS CODE\\BlackCoffer-TestAssginment\\20211030 Test Assignment\\Output.csv')\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
